{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Investment Success Prediction Model Comparison\n",
        "\n",
        "This notebook compares multiple machine learning algorithms to predict investment success probability.\n",
        "\n",
        "## Models to Compare:\n",
        "- Linear Regression\n",
        "- Ridge Regression\n",
        "- Lasso Regression\n",
        "- Decision Tree Regressor\n",
        "- Gradient Boosting Regressor\n",
        "- XGBoost Regressor\n",
        "- Random Forest Regressor\n",
        "- Support Vector Regressor\n",
        "- Neural Network (MLPRegressor)\n",
        "\n",
        "## Evaluation Metrics:\n",
        "- Mean Absolute Error (MAE)\n",
        "- Mean Squared Error (MSE)\n",
        "- Root Mean Squared Error (RMSE)\n",
        "- R¬≤ Score\n",
        "- Mean Absolute Percentage Error (MAPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning imports\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "from scipy import stats\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print('Libraries imported successfully!')\n",
        "print(f'Analysis started at: {datetime.now()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "print('Loading dataset...')\n",
        "# File path and setup\n",
        "DATA_PATH = \"/investa/datasets/v1.csv\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    print(\"File not found. Please upload 'datasets/v1.csv'\")\n",
        "    uploaded = files.upload()\n",
        "    os.makedirs(\"/investa\", exist_ok=True)\n",
        "    for filename in uploaded:\n",
        "        uploaded_path = f\"/investa/{filename}\"\n",
        "        os.rename(filename, uploaded_path)\n",
        "        print(f\"Saved file to {uploaded_path}\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Basic information about the dataset\n",
        "print(f'Dataset shape: {df.shape}')\n",
        "print(f'\\nDataset info:')\n",
        "print(df.info())\n",
        "\n",
        "# Display first few rows\n",
        "print('\\nFirst 5 rows:')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print('Missing values per column:')\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Statistical summary\n",
        "print('\\nStatistical Summary:')\n",
        "display(df.describe())\n",
        "\n",
        "# Check categorical columns unique values\n",
        "categorical_cols = ['industry', 'target_market', 'business_model', 'traction']\n",
        "print('\\nCategorical columns unique values:')\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        print(f'{col}: {df[col].unique()}')\n",
        "        print(f'  Count: {len(df[col].unique())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize target variable distribution\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(df['success_probability'], bins=50, alpha=0.7, color='skyblue')\n",
        "plt.title('Distribution of Success Probability')\n",
        "plt.xlabel('Success Probability')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.boxplot(df['success_probability'])\n",
        "plt.title('Boxplot of Success Probability')\n",
        "plt.ylabel('Success Probability')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "stats.probplot(df['success_probability'], dist='norm', plot=plt)\n",
        "plt.title('Q-Q Plot of Success Probability')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'Target variable statistics:')\n",
        "print(f'Mean: {df[\"success_probability\"].mean():.4f}')\n",
        "print(f'Std: {df[\"success_probability\"].std():.4f}')\n",
        "print(f'Min: {df[\"success_probability\"].min():.4f}')\n",
        "print(f'Max: {df[\"success_probability\"].max():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['industry', 'target_market', 'business_model', 'traction']\n",
        "# Filter to only include columns that actually exist in the dataset\n",
        "categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
        "numerical_cols = [col for col in df.columns if col not in categorical_cols and col != 'success_probability']\n",
        "\n",
        "print(f'Categorical columns: {categorical_cols}')\n",
        "print(f'Numerical columns: {numerical_cols}')\n",
        "\n",
        "# Create a copy for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Initialize label encoders dictionary\n",
        "label_encoders = {}\n",
        "\n",
        "# Encode categorical variables BEFORE splitting\n",
        "for col in categorical_cols:\n",
        "    print(f'\\nProcessing {col}...')\n",
        "    print(f'Unique values: {df_processed[col].unique()}')\n",
        "    \n",
        "    le = LabelEncoder()\n",
        "    df_processed[col] = le.fit_transform(df_processed[col].astype(str))  # Convert to string first\n",
        "    label_encoders[col] = le\n",
        "    \n",
        "    print(f'Encoded {col}: {list(le.classes_)}')\n",
        "    print(f'Encoded values: {df_processed[col].unique()}')\n",
        "\n",
        "print('\\nCategorical encoding completed!')\n",
        "print(f'Processed dataset shape: {df_processed.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X = df_processed.drop('success_probability', axis=1)\n",
        "y = df_processed['success_probability']\n",
        "\n",
        "print(f'Feature matrix shape: {X.shape}')\n",
        "print(f'Target vector shape: {y.shape}')\n",
        "print(f'Features: {list(X.columns)}')\n",
        "\n",
        "# Check for any remaining non-numeric data\n",
        "print('\\nData types check:')\n",
        "print(X.dtypes)\n",
        "\n",
        "# Split the data with stratification based on success probability bins\n",
        "# Create bins for stratification\n",
        "y_binned = pd.cut(y, bins=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y_binned\n",
        ")\n",
        "\n",
        "print(f'\\nTrain set: {X_train.shape[0]} samples')\n",
        "print(f'Test set: {X_test.shape[0]} samples')\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print('\\nFeature scaling completed!')\n",
        "print(f'Scaled train set shape: {X_train_scaled.shape}')\n",
        "print(f'Scaled test set shape: {X_test_scaled.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Definition and Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define evaluation metrics function\n",
        "def calculate_metrics(y_true, y_pred, model_name):\n",
        "    '''Calculate and return various regression metrics'''\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    # Mean Absolute Percentage Error (MAPE) with protection against division by zero\n",
        "    # Replace zeros with small values to avoid division by zero\n",
        "    y_true_safe = np.where(y_true == 0, 1e-10, y_true)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100\n",
        "    \n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'MAE': mae,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'R¬≤': r2,\n",
        "        'MAPE': mape\n",
        "    }\n",
        "\n",
        "# Define cross-validation function\n",
        "def perform_cv(model, X, y, cv=5):\n",
        "    '''Perform cross-validation and return scores'''\n",
        "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='r2')\n",
        "    return cv_scores\n",
        "\n",
        "print('Utility functions defined!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models dictionary with more conservative parameters\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
        "    'Lasso Regression': Lasso(alpha=0.01, random_state=42, max_iter=2000),\n",
        "    'Decision Tree': DecisionTreeRegressor(random_state=42, max_depth=10, min_samples_split=5),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10, min_samples_split=5),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=6, learning_rate=0.1),\n",
        "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, max_depth=6, learning_rate=0.1),\n",
        "    'SVR': SVR(kernel='rbf', C=1.0, gamma='scale'),\n",
        "    'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), random_state=42, max_iter=1000, early_stopping=True)\n",
        "}\n",
        "\n",
        "print(f'Initialized {len(models)} models for comparison')\n",
        "for name in models.keys():\n",
        "    print(f'- {name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results\n",
        "results = []\n",
        "trained_models = {}\n",
        "cv_results = {}\n",
        "\n",
        "print('Training and evaluating models...\\n')\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f'Training {name}...')\n",
        "    \n",
        "    try:\n",
        "        # Use scaled data for models that need it\n",
        "        if name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'SVR', 'Neural Network']:\n",
        "            X_train_use = X_train_scaled\n",
        "            X_test_use = X_test_scaled\n",
        "        else:\n",
        "            X_train_use = X_train.values  # Convert to numpy array to avoid issues\n",
        "            X_test_use = X_test.values\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(X_train_use, y_train)\n",
        "        \n",
        "        # Make predictions\n",
        "        y_train_pred = model.predict(X_train_use)\n",
        "        y_test_pred = model.predict(X_test_use)\n",
        "        \n",
        "        # Calculate metrics for training and test sets\n",
        "        train_metrics = calculate_metrics(y_train, y_train_pred, f'{name} (Train)')\n",
        "        test_metrics = calculate_metrics(y_test, y_test_pred, f'{name} (Test)')\n",
        "        \n",
        "        # Perform cross-validation\n",
        "        cv_scores = perform_cv(model, X_train_use, y_train)\n",
        "        cv_results[name] = cv_scores\n",
        "        \n",
        "        # Store results\n",
        "        results.append(train_metrics)\n",
        "        results.append(test_metrics)\n",
        "        \n",
        "        # Store trained model\n",
        "        trained_models[name] = model\n",
        "        \n",
        "        print(f'‚úì {name} completed - Test R¬≤: {test_metrics[\"R¬≤\"]:.4f}')\n",
        "        print(f'  CV R¬≤ Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\\n')\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Error training {name}: {str(e)}\\n')\n",
        "        continue\n",
        "\n",
        "print('Model training completed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Separate train and test results\n",
        "train_results = results_df[results_df['Model'].str.contains('Train')].copy()\n",
        "test_results = results_df[results_df['Model'].str.contains('Test')].copy()\n",
        "\n",
        "# Clean model names\n",
        "train_results['Model'] = train_results['Model'].str.replace(' (Train)', '')\n",
        "test_results['Model'] = test_results['Model'].str.replace(' (Test)', '')\n",
        "\n",
        "print('Model Performance Summary (Test Set):')\n",
        "print('=' * 60)\n",
        "test_results_sorted = test_results.sort_values('R¬≤', ascending=False)\n",
        "for _, row in test_results_sorted.iterrows():\n",
        "    print(f'{row[\"Model\"]:<20} | R¬≤: {row[\"R¬≤\"]:.4f} | RMSE: {row[\"RMSE\"]:.4f} | MAE: {row[\"MAE\"]:.4f}')\n",
        "\n",
        "# Display detailed results table\n",
        "print('\\nDetailed Results:')\n",
        "display(test_results_sorted.round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# R¬≤ Score comparison\n",
        "axes[0, 0].barh(test_results['Model'], test_results['R¬≤'], color='skyblue')\n",
        "axes[0, 0].set_title('R¬≤ Score Comparison (Test Set)')\n",
        "axes[0, 0].set_xlabel('R¬≤ Score')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# RMSE comparison\n",
        "axes[0, 1].barh(test_results['Model'], test_results['RMSE'], color='lightcoral')\n",
        "axes[0, 1].set_title('RMSE Comparison (Test Set)')\n",
        "axes[0, 1].set_xlabel('RMSE')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# MAE comparison\n",
        "axes[1, 0].barh(test_results['Model'], test_results['MAE'], color='lightgreen')\n",
        "axes[1, 0].set_title('MAE Comparison (Test Set)')\n",
        "axes[1, 0].set_xlabel('MAE')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# MAPE comparison\n",
        "axes[1, 1].barh(test_results['Model'], test_results['MAPE'], color='orange')\n",
        "axes[1, 1].set_title('MAPE Comparison (Test Set)')\n",
        "axes[1, 1].set_xlabel('MAPE (%)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Best Model Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify the best model based on test R¬≤ score\n",
        "best_model_name = test_results.loc[test_results['R¬≤'].idxmax(), 'Model']\n",
        "best_model = trained_models[best_model_name]\n",
        "best_scores = test_results[test_results['Model'] == best_model_name].iloc[0]\n",
        "\n",
        "print(f'üèÜ BEST MODEL: {best_model_name}')\n",
        "print('=' * 50)\n",
        "print(f'R¬≤ Score: {best_scores[\"R¬≤\"]:.4f}')\n",
        "print(f'RMSE: {best_scores[\"RMSE\"]:.4f}')\n",
        "print(f'MAE: {best_scores[\"MAE\"]:.4f}')\n",
        "print(f'MAPE: {best_scores[\"MAPE\"]:.4f}%')\n",
        "print(f'CV Mean R¬≤: {cv_results[best_model_name].mean():.4f}')\n",
        "print(f'CV Std R¬≤: {cv_results[best_model_name].std():.4f}')\n",
        "\n",
        "# Get predictions from best model\n",
        "if best_model_name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'SVR', 'Neural Network']:\n",
        "    y_pred_best = best_model.predict(X_test_scaled)\n",
        "else:\n",
        "    y_pred_best = best_model.predict(X_test.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize best model performance\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Predicted vs Actual\n",
        "axes[0, 0].scatter(y_test, y_pred_best, alpha=0.6, color='blue')\n",
        "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[0, 0].set_xlabel('Actual Success Probability')\n",
        "axes[0, 0].set_ylabel('Predicted Success Probability')\n",
        "axes[0, 0].set_title(f'{best_model_name}: Predicted vs Actual')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals plot\n",
        "residuals = y_test - y_pred_best\n",
        "axes[0, 1].scatter(y_pred_best, residuals, alpha=0.6, color='green')\n",
        "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
        "axes[0, 1].set_xlabel('Predicted Success Probability')\n",
        "axes[0, 1].set_ylabel('Residuals')\n",
        "axes[0, 1].set_title(f'{best_model_name}: Residuals Plot')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals histogram\n",
        "axes[1, 0].hist(residuals, bins=30, alpha=0.7, color='purple')\n",
        "axes[1, 0].set_xlabel('Residuals')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title(f'{best_model_name}: Residuals Distribution')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Q-Q plot of residuals\n",
        "stats.probplot(residuals, dist='norm', plot=axes[1, 1])\n",
        "axes[1, 1].set_title(f'{best_model_name}: Q-Q Plot of Residuals')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance for tree-based models\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
        "    plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title(f'{best_model_name}: Feature Importance')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print('Top 10 Most Important Features:')\n",
        "    display(feature_importance.head(10))\n",
        "    \n",
        "elif hasattr(best_model, 'coef_'):\n",
        "    # For linear models, show coefficient magnitudes\n",
        "    feature_coef = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'coefficient': best_model.coef_,\n",
        "        'abs_coefficient': np.abs(best_model.coef_)\n",
        "    }).sort_values('abs_coefficient', ascending=False)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = ['red' if x < 0 else 'blue' for x in feature_coef['coefficient']]\n",
        "    plt.barh(range(len(feature_coef)), feature_coef['coefficient'], color=colors)\n",
        "    plt.yticks(range(len(feature_coef)), feature_coef['feature'])\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.title(f'{best_model_name}: Feature Coefficients')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print('Top 10 Most Important Features (by coefficient magnitude):')\n",
        "    display(feature_coef.head(10))\n",
        "else:\n",
        "    print(f'Feature importance not available for {best_model_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Saving and Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model and preprocessors\n",
        "import os\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "\n",
        "# Save the best model\n",
        "model_filename = f'saved_models/best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
        "joblib.dump(best_model, model_filename)\n",
        "\n",
        "# Save the scaler\n",
        "scaler_filename = 'saved_models/feature_scaler.pkl'\n",
        "joblib.dump(scaler, scaler_filename)\n",
        "\n",
        "# Save label encoders\n",
        "encoders_filename = 'saved_models/label_encoders.pkl'\n",
        "joblib.dump(label_encoders, encoders_filename)\n",
        "\n",
        "# Save feature names\n",
        "features_filename = 'saved_models/feature_names.pkl'\n",
        "joblib.dump(list(X.columns), features_filename)\n",
        "\n",
        "# Save categorical columns list\n",
        "categorical_cols_filename = 'saved_models/categorical_columns.pkl'\n",
        "joblib.dump(categorical_cols, categorical_cols_filename)\n",
        "\n",
        "# Save model performance metrics\n",
        "metrics_filename = 'saved_models/model_metrics.pkl'\n",
        "model_info = {\n",
        "    'best_model_name': best_model_name,\n",
        "    'test_metrics': best_scores.to_dict(),\n",
        "    'cv_scores': cv_results[best_model_name],\n",
        "    'all_results': results_df,\n",
        "    'categorical_columns': categorical_cols\n",
        "}\n",
        "joblib.dump(model_info, metrics_filename)\n",
        "\n",
        "print(f'‚úÖ Best model saved: {model_filename}')\n",
        "print(f'‚úÖ Scaler saved: {scaler_filename}')\n",
        "print(f'‚úÖ Label encoders saved: {encoders_filename}')\n",
        "print(f'‚úÖ Feature names saved: {features_filename}')\n",
        "print(f'‚úÖ Categorical columns saved: {categorical_cols_filename}')\n",
        "print(f'‚úÖ Model metrics saved: {metrics_filename}')\n",
        "\n",
        "print(f'\\nüìÅ All files saved in \"saved_models\" directory')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Robust Model Loading and Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a robust prediction function for new data\n",
        "def load_model_and_predict(new_data):\n",
        "    '''\n",
        "    Load the saved model and make predictions on new data\n",
        "    \n",
        "    Parameters:\n",
        "    new_data: pandas DataFrame with the same columns as training data (except success_probability)\n",
        "    \n",
        "    Returns:\n",
        "    predictions: numpy array of predicted success probabilities\n",
        "    '''\n",
        "    \n",
        "    # Load saved components\n",
        "    model = joblib.load(model_filename)\n",
        "    scaler = joblib.load(scaler_filename)\n",
        "    encoders = joblib.load(encoders_filename)\n",
        "    feature_names = joblib.load(features_filename)\n",
        "    categorical_columns = joblib.load(categorical_cols_filename)\n",
        "    \n",
        "    # Preprocess new data\n",
        "    new_data_processed = new_data.copy()\n",
        "    \n",
        "    # Encode categorical variables with robust handling for unseen categories\n",
        "    for col in categorical_columns:\n",
        "        if col in new_data_processed.columns:\n",
        "            encoder = encoders[col]\n",
        "            \n",
        "            # Convert to string to ensure consistent data type\n",
        "            new_data_processed[col] = new_data_processed[col].astype(str)\n",
        "            \n",
        "            # Handle unseen categories by mapping them to the most frequent class\n",
        "            def safe_transform(value):\n",
        "                try:\n",
        "                    return encoder.transform([value])[0]\n",
        "                except ValueError:\n",
        "                    # If unseen category, use the most frequent class (first in classes_)\n",
        "                    most_frequent_class = encoder.classes_[0]\n",
        "                    print(f'Warning: Unseen category \"{value}\" in column \"{col}\". Using \"{most_frequent_class}\".')\n",
        "                    return encoder.transform([most_frequent_class])[0]\n",
        "            \n",
        "            new_data_processed[col] = new_data_processed[col].apply(safe_transform)\n",
        "    \n",
        "    # Ensure correct feature order and fill missing features with 0\n",
        "    for feature in feature_names:\n",
        "        if feature not in new_data_processed.columns:\n",
        "            new_data_processed[feature] = 0\n",
        "            print(f'Warning: Missing feature \"{feature}\". Filled with 0.')\n",
        "    \n",
        "    # Select only the required features in correct order\n",
        "    new_data_processed = new_data_processed[feature_names]\n",
        "    \n",
        "    # Scale features if needed\n",
        "    if best_model_name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'SVR', 'Neural Network']:\n",
        "        new_data_processed = scaler.transform(new_data_processed)\n",
        "    else:\n",
        "        new_data_processed = new_data_processed.values\n",
        "    \n",
        "    # Make predictions\n",
        "    predictions = model.predict(new_data_processed)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "print('Robust prediction function defined!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the prediction function with sample data from the original dataset\n",
        "# Get some test samples\n",
        "sample_indices = df.sample(3, random_state=42).index\n",
        "sample_data_original = df.loc[sample_indices].drop('success_probability', axis=1)\n",
        "sample_actual = df.loc[sample_indices]['success_probability'].values\n",
        "\n",
        "print('Testing prediction function with sample data:')\n",
        "print('Sample data:')\n",
        "display(sample_data_original)\n",
        "\n",
        "# Test predictions\n",
        "sample_predictions = load_model_and_predict(sample_data_original)\n",
        "\n",
        "print('\\nPrediction Results:')\n",
        "print('=' * 50)\n",
        "for i in range(len(sample_predictions)):\n",
        "    print(f'Sample {i+1}:')\n",
        "    print(f'  Predicted: {sample_predictions[i]:.4f}')\n",
        "    print(f'  Actual:    {sample_actual[i]:.4f}')\n",
        "    print(f'  Difference: {abs(sample_predictions[i] - sample_actual[i]):.4f}')\n",
        "    print()\n",
        "\n",
        "print('‚úÖ Prediction function working correctly!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Final Summary and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print('üéØ INVESTMENT PREDICTION MODEL ANALYSIS SUMMARY')\n",
        "print('=' * 60)\n",
        "print(f'Dataset: {df.shape[0]} samples, {df.shape[1]-1} features')\n",
        "print(f'Target: Success Probability (Range: {df[\"success_probability\"].min():.4f} - {df[\"success_probability\"].max():.4f})')\n",
        "print(f'\\nüèÜ BEST MODEL: {best_model_name}')\n",
        "print(f'   R¬≤ Score: {best_scores[\"R¬≤\"]:.4f} (Explains {best_scores[\"R¬≤\"]*100:.1f}% of variance)')\n",
        "print(f'   RMSE: {best_scores[\"RMSE\"]:.4f}')\n",
        "print(f'   MAE: {best_scores[\"MAE\"]:.4f}')\n",
        "print(f'   Cross-Validation R¬≤: {cv_results[best_model_name].mean():.4f} ¬± {cv_results[best_model_name].std():.4f}')\n",
        "\n",
        "print('\\nüìä TOP 3 MODELS BY PERFORMANCE:')\n",
        "top_3 = test_results_sorted.head(3)\n",
        "for i, (_, row) in enumerate(top_3.iterrows(), 1):\n",
        "    print(f'   {i}. {row[\"Model\"]:<20} | R¬≤: {row[\"R¬≤\"]:.4f} | RMSE: {row[\"RMSE\"]:.4f}')\n",
        "\n",
        "print('\\nüíæ SAVED ARTIFACTS:')\n",
        "print(f'   ‚Ä¢ Best model: {model_filename}')\n",
        "print(f'   ‚Ä¢ Feature scaler: {scaler_filename}')\n",
        "print(f'   ‚Ä¢ Label encoders: {encoders_filename}')\n",
        "print(f'   ‚Ä¢ Feature names: {features_filename}')\n",
        "print(f'   ‚Ä¢ Categorical columns: {categorical_cols_filename}')\n",
        "print(f'   ‚Ä¢ Model metrics: {metrics_filename}')\n",
        "\n",
        "print('\\nüîÆ TO USE THE MODEL FOR NEW PREDICTIONS:')\n",
        "print('   1. Prepare data with columns: {}'.format(', '.join(list(df.columns)[:-1])))\n",
        "print('   2. Call load_model_and_predict(new_data_df)')\n",
        "print('   3. The function handles encoding and scaling automatically')\n",
        "print('   4. Returns success probability predictions')\n",
        "\n",
        "print('\\nüìã DATA REQUIREMENTS:')\n",
        "print(f'   ‚Ä¢ Categorical columns: {categorical_cols}')\n",
        "print(f'   ‚Ä¢ Numerical columns: {[col for col in df.columns if col not in categorical_cols and col != \"success_probability\"]}')\n",
        "\n",
        "print(f'\\n‚úÖ Analysis completed at: {datetime.now()}')\n",
        "print('\\nüéâ Ready for production use!')\n",
        "print('\\nüõ°Ô∏è The model includes robust error handling for:')\n",
        "print('   ‚Ä¢ Unseen categorical values')\n",
        "print('   ‚Ä¢ Missing features')\n",
        "print('   ‚Ä¢ Data type inconsistencies')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Demonstration: Using the Model for New Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstration: How to use the model for new predictions\n",
        "print('üîÆ DEMONSTRATION: Model Loading and Prediction')\n",
        "print('=' * 60)\n",
        "\n",
        "# Create 5 example investment scenarios for prediction\n",
        "example_data = pd.DataFrame({\n",
        "    'industry': ['Technology', 'Health and Fitness', 'Fashion', 'Food and Beverage', 'Beauty'],\n",
        "    'funding_egp': [500000, 1200000, 300000, 800000, 450000],\n",
        "    'equity_percentage': [25.0, 15.0, 30.0, 20.0, 35.0],\n",
        "    'duration_months': [12, 18, 6, 24, 9],\n",
        "    'target_market': ['Global', 'Regional', 'Local', 'Global', 'Regional'],\n",
        "    'business_model': ['B2B', 'Marketplace', 'Subscription', 'B2B', 'Marketplace'],\n",
        "    'founder_experience_years': [8, 12, 3, 15, 6],\n",
        "    'team_size': [5, 8, 3, 12, 4],\n",
        "    'traction': ['High', 'Medium', 'Low', 'High', 'Medium'],\n",
        "    'market_size_usd': [10000000, 7000000, 5000000, 8000000, 6000000],\n",
        "    'funding_usd': [16666.67, 40000.0, 10000.0, 26666.67, 15000.0],\n",
        "    'profit': [120000, 85000, -15000, 180000, 45000],\n",
        "    'repeat_purchase_rate': [0.7, 0.6, 0.3, 0.8, 0.5],\n",
        "    'branches_count': [2, 3, 0, 5, 1],\n",
        "    'revenue': [450000, 320000, 85000, 720000, 180000],\n",
        "    'customers': [1500, 2200, 450, 3500, 800],\n",
        "    'revenue_growth': [0.25, 0.18, 0.05, 0.32, 0.15],\n",
        "    'profit_margin': [0.27, 0.27, -0.18, 0.25, 0.25],\n",
        "    'customer_growth': [0.20, 0.15, 0.08, 0.28, 0.12],\n",
        "    'churn_rate': [0.15, 0.20, 0.35, 0.12, 0.25],\n",
        "    'operating_costs': [330000, 235000, 100000, 540000, 135000],\n",
        "    'debt_to_profit_ratio': [0.8, 1.2, -2.5, 0.6, 1.5]\n",
        "})\n",
        "\n",
        "print('üìä Example Investment Scenarios:')\n",
        "print('-' * 60)\n",
        "display(example_data)\n",
        "\n",
        "print('\\nüéØ Making Predictions...')\n",
        "print('-' * 60)\n",
        "\n",
        "# Use the prediction function\n",
        "try:\n",
        "    predictions = load_model_and_predict(example_data)\n",
        "    \n",
        "    # Create results dataframe\n",
        "    results_demo = pd.DataFrame({\n",
        "        'Scenario': [f'Investment {i+1}' for i in range(len(predictions))],\n",
        "        'Industry': example_data['industry'],\n",
        "        'Funding (EGP)': example_data['funding_egp'],\n",
        "        'Business Model': example_data['business_model'],\n",
        "        'Traction': example_data['traction'],\n",
        "        'Predicted Success Probability': [f'{pred:.4f}' for pred in predictions],\n",
        "        'Success Percentage': [f'{pred*100:.2f}%' for pred in predictions],\n",
        "        'Risk Level': ['High Risk' if pred < 0.3 else 'Medium Risk' if pred < 0.7 else 'Low Risk' for pred in predictions]\n",
        "    })\n",
        "    \n",
        "    print('üìà PREDICTION RESULTS:')\n",
        "    print('=' * 80)\n",
        "    display(results_demo)\n",
        "    \n",
        "    # Show detailed analysis for each prediction\n",
        "    print('\\nüìã DETAILED ANALYSIS:')\n",
        "    print('=' * 80)\n",
        "    for i, (_, row) in enumerate(results_demo.iterrows()):\n",
        "        risk_emoji = 'üî¥' if predictions[i] < 0.3 else 'üü°' if predictions[i] < 0.7 else 'üü¢'\n",
        "        print(f'{risk_emoji} {row[\"Scenario\"]} - {row[\"Industry\"]} ({row[\"Business Model\"]})')\n",
        "        print(f'   Success Probability: {predictions[i]:.4f} ({predictions[i]*100:.2f}%)')\n",
        "        print(f'   Funding: {row[\"Funding (EGP)\"]:,} EGP | Traction: {row[\"Traction\"]} | Risk: {row[\"Risk Level\"]}')\n",
        "        print()\n",
        "    \n",
        "    # Investment recommendations\n",
        "    print('\\nüí° INVESTMENT RECOMMENDATIONS:')\n",
        "    print('=' * 80)\n",
        "    \n",
        "    high_potential = results_demo[results_demo['Risk Level'] == 'Low Risk']\n",
        "    medium_potential = results_demo[results_demo['Risk Level'] == 'Medium Risk']\n",
        "    low_potential = results_demo[results_demo['Risk Level'] == 'High Risk']\n",
        "    \n",
        "    if len(high_potential) > 0:\n",
        "        print('üü¢ RECOMMENDED INVESTMENTS (Low Risk):')\n",
        "        for _, row in high_potential.iterrows():\n",
        "            print(f'   ‚Ä¢ {row[\"Scenario\"]}: {row[\"Industry\"]} - {row[\"Success Percentage\"]} success rate')\n",
        "        print()\n",
        "    \n",
        "    if len(medium_potential) > 0:\n",
        "        print('üü° CONSIDER WITH CAUTION (Medium Risk):')\n",
        "        for _, row in medium_potential.iterrows():\n",
        "            print(f'   ‚Ä¢ {row[\"Scenario\"]}: {row[\"Industry\"]} - {row[\"Success Percentage\"]} success rate')\n",
        "        print()\n",
        "    \n",
        "    if len(low_potential) > 0:\n",
        "        print('üî¥ NOT RECOMMENDED (High Risk):')\n",
        "        for _, row in low_potential.iterrows():\n",
        "            print(f'   ‚Ä¢ {row[\"Scenario\"]}: {row[\"Industry\"]} - {row[\"Success Percentage\"]} success rate')\n",
        "        print()\n",
        "    \n",
        "    # Summary statistics\n",
        "    print('üìä SUMMARY STATISTICS:')\n",
        "    print('=' * 80)\n",
        "    print(f'Average Success Probability: {predictions.mean():.4f} ({predictions.mean()*100:.2f}%)')\n",
        "    print(f'Highest Success Probability: {predictions.max():.4f} ({predictions.max()*100:.2f}%)')\n",
        "    print(f'Lowest Success Probability: {predictions.min():.4f} ({predictions.min()*100:.2f}%)')\n",
        "    print(f'Standard Deviation: {predictions.std():.4f}')\n",
        "    print(f'\\nRisk Distribution:')\n",
        "    print(f'  üü¢ Low Risk (>70%): {len(high_potential)} investments')\n",
        "    print(f'  üü° Medium Risk (30-70%): {len(medium_potential)} investments')\n",
        "    print(f'  üî¥ High Risk (<30%): {len(low_potential)} investments')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f'‚ùå Error making predictions: {str(e)}')\n",
        "    print('Make sure the model has been trained and saved first!')\n",
        "\n",
        "print('\\n‚úÖ Demonstration completed!')\n",
        "print('\\nüí° TIP: You can modify the example_data DataFrame above to test different scenarios')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. How to Use This Model in Production\n",
        "\n",
        "### Step-by-Step Guide:\n",
        "\n",
        "1. **Prepare your data** in the same format as the training data\n",
        "2. **Load the model** using the `load_model_and_predict()` function\n",
        "3. **Get predictions** for success probability\n",
        "4. **Make investment decisions** based on the results\n",
        "\n",
        "### Data Requirements:\n",
        "- All numerical features must be present\n",
        "- Categorical features: `industry`, `target_market`, `business_model`, `traction`\n",
        "- The function handles missing or unknown categories automatically\n",
        "\n",
        "### Example Usage:\n",
        "```python\n",
        "# Your new investment data\n",
        "new_investment = pd.DataFrame({\n",
        "    'industry': ['Technology'],\n",
        "    'funding_egp': [750000],\n",
        "    'equity_percentage': [20.0],\n",
        "    # ... include all other required columns\n",
        "})\n",
        "\n",
        "# Get prediction\n",
        "success_prob = load_model_and_predict(new_investment)\n",
        "print(f'Success probability: {success_prob[0]:.4f}')\n",
        "```\n",
        "\n",
        "### Interpretation Guide:\n",
        "- **> 0.70**: Low risk, high chance of success\n",
        "- **0.30 - 0.70**: Medium risk, requires careful evaluation\n",
        "- **< 0.30**: High risk, not recommended without major improvements"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
